{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# Cryptocurrencies with Market Cap +$1B : EDA and Clustering","metadata":{}},{"cell_type":"markdown","source":"## Acknowledgements\n\n* dataset [Forecasting Top Cryptocurrencies](https://www.kaggle.com/datasets/vbmokin/forecasting-top-cryptocurrencies)\n* notebook [Time Series Clustering [Store Sales]](https://www.kaggle.com/code/raskoshik/time-series-clustering-store-sales)\n* notebook [Introduction to Time Series Clustering](https://www.kaggle.com/code/izzettunc/introduction-to-time-series-clustering/notebook)","metadata":{}},{"cell_type":"markdown","source":"## Intro\n\n### Let's cluster and study the patterns of cryptocurrencies in 2021 with a capitalization of more than $1 billion now.\n\n### Let's take notebook [Time Series Clustering [Store Sales]](https://www.kaggle.com/code/raskoshik/time-series-clustering-store-sales) as a basis and adapt it for our task\n\n## Content \n- <a href='#1'>1. Data Description</a>\n- <a href='#2'>2. Dealing with Missing Data</a>\n- <a href='#3'>3. Time Series Feature Extraction</a>\n- <a href='#4'>4. Clustering Methods</a>\n    - <a href='#4.1'>4.1 Time Series Smoothing</a>\n    - <a href='#4.2'>4.2 Time Series Scaling</a>\n    - <a href='#4.3'>4.3 Time Series K-Means</a>\n    - <a href='#4.4'>4.4 Downsizing Feature Space</a>\n        - <a href='#4.4.1'>4.4.1 t-SNE</a>\n        - <a href='#4.4.2'>4.4.2 MultiDimensional Scaling (MDS)</a>\n    - <a href='#4.5'>4.5 Hierarchical Agglomerative Clustering (HAC)</a>\n    - <a href='#4.6'>4.6 Time Series KMeans Results</a>\n- <a href='#5'>5. Cluster Series Extraction</a>\n    - <a href='#5.1'>5.1 Cluster Series DBA</a>\n- <a href='#6'>6. Time Series Embeddings</a>\n- <a href='#7'>7. References</a>","metadata":{}},{"cell_type":"code","source":"# Some libraries installation\n! git clone https://github.com/tejaslodaya/timeseries-clustering-vae.git\n! pip install tslearn\n! pip uninstall scikit-learn --yes \n! pip install scikit-learn==0.24.1","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:23:44.684054Z","iopub.execute_input":"2022-04-17T14:23:44.684354Z","iopub.status.idle":"2022-04-17T14:23:55.003682Z","shell.execute_reply.started":"2022-04-17T14:23:44.68432Z","shell.execute_reply":"2022-04-17T14:23:55.002727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport os\n\nfrom tslearn.clustering import TimeSeriesKMeans\nfrom tslearn.barycenters import dtw_barycenter_averaging\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.manifold import TSNE, MDS\nfrom sklearn.cluster import AgglomerativeClustering\n\nfrom scipy.cluster.hierarchy import dendrogram\nfrom tqdm.autonotebook import tqdm\n\nwarnings.filterwarnings(\"ignore\")\nsns.set_style(\"darkgrid\")\n\nSEED=42","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:23:55.005923Z","iopub.execute_input":"2022-04-17T14:23:55.00625Z","iopub.status.idle":"2022-04-17T14:23:55.015238Z","shell.execute_reply.started":"2022-04-17T14:23:55.006203Z","shell.execute_reply":"2022-04-17T14:23:55.01412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\nimport requests\nimport pandas_datareader as web\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:23:55.01675Z","iopub.execute_input":"2022-04-17T14:23:55.016968Z","iopub.status.idle":"2022-04-17T14:23:55.030725Z","shell.execute_reply.started":"2022-04-17T14:23:55.01694Z","shell.execute_reply":"2022-04-17T14:23:55.029975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set time interval of data for given cryptocurrency : 2021 - the last full year\ndate_start = datetime.datetime(2021, 1, 1)\n# date_end = datetime.datetime.now()\ndate_end = datetime.datetime(2021, 12, 31)\nprint(f\"Time interval: from {date_start} to {date_end}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:23:55.033148Z","iopub.execute_input":"2022-04-17T14:23:55.033392Z","iopub.status.idle":"2022-04-17T14:23:55.045047Z","shell.execute_reply.started":"2022-04-17T14:23:55.033361Z","shell.execute_reply":"2022-04-17T14:23:55.043331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='1'>1. Data Description</a>\n\nSo in total, we have 83 cryptocurrencies with +$1B Market Cap - see dataset [Forecasting Top Cryptocurrencies](https://www.kaggle.com/datasets/vbmokin/forecasting-top-cryptocurrencies)\n\nWe cluster them. That can help to:\n- Find similar patterns in/between cryptocurrencies\n- Reduce the number of models to be trained (cluster models)\n- ...\n\n**We are going to find similar cryptocurrencies**","metadata":{}},{"cell_type":"code","source":"# Data Reading \ndf_about = pd.read_csv('../input/forecasting-top-cryptocurrencies/about_top_cryptocurrencies_1B_information.csv', sep=\";\")\ndf_about.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:23:55.046102Z","iopub.execute_input":"2022-04-17T14:23:55.046368Z","iopub.status.idle":"2022-04-17T14:23:55.081309Z","shell.execute_reply.started":"2022-04-17T14:23:55.046337Z","shell.execute_reply":"2022-04-17T14:23:55.080037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get list of the code of all cryptocurrencies in this dataset\ncrypto_codes_list = df_about['code'].tolist()\nnp.array(crypto_codes_list)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:23:55.083236Z","iopub.execute_input":"2022-04-17T14:23:55.083557Z","iopub.status.idle":"2022-04-17T14:23:55.092012Z","shell.execute_reply.started":"2022-04-17T14:23:55.08352Z","shell.execute_reply":"2022-04-17T14:23:55.091341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data download via API\ndef get_data_codes(cryptocurrencies_list, col, date_start, date_end=None):\n    # Get feature col for given cryptocurrency in USD from Yahoo.finance and https://coinmarketcap.com/\n    # col is the 'High', 'Low', 'Open', 'Close' or 'Volume' only!\n    # date_end = None means that the date_end is the current day\n    \n    # Check for col\n    if col not in ['High', 'Low', 'Open', 'Close', 'Volume']:\n        print(f\"Feature {col} is absent\")\n        return None\n    \n    # Check for date of the end\n    if date_end is None:\n        date_end = dt.datetime.now()\n    \n    # Generate the DataFrame with the list of dates\n    data = pd.DataFrame()\n    dates_list = []\n    for i in range((date_end - date_start).days + 1):\n        #dates_list.append((date_start + datetime.timedelta(i)).strftime(\"%Y-%m-%d\"))\n        dates_list.append(date_start + datetime.timedelta(i))\n    data['date'] = dates_list\n    \n    # Get data\n    for item in cryptocurrencies_list:\n        \n        # Download data\n        try:\n            df = web.DataReader(f'{item}-USD', 'yahoo', date_start, date_end)\n            df = df[[col]].reset_index(drop=False)\n            df.columns = ['date', item]\n\n            # Merging data\n            data = data.merge(df, on='date', how='left')\n            #print(item)            \n        \n        except:\n            print(f'Cryptocurrency \"{item}\" has problem downloading from Yahoo')\n        \n    return data","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:23:55.093314Z","iopub.execute_input":"2022-04-17T14:23:55.093725Z","iopub.status.idle":"2022-04-17T14:23:55.104467Z","shell.execute_reply.started":"2022-04-17T14:23:55.093691Z","shell.execute_reply":"2022-04-17T14:23:55.103421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#df = get_data_codes(['BTC', 'ICP', 'GALA'], 'Close', date_start, date_end)\ndf = get_data_codes(crypto_codes_list, 'Close', date_start, date_end)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:23:55.106038Z","iopub.execute_input":"2022-04-17T14:23:55.106482Z","iopub.status.idle":"2022-04-17T14:25:29.457235Z","shell.execute_reply.started":"2022-04-17T14:23:55.106445Z","shell.execute_reply":"2022-04-17T14:25:29.456497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of cryptocurrencies (without \"ICP\"): ', df.shape[1]-1)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:29.458274Z","iopub.execute_input":"2022-04-17T14:25:29.458452Z","iopub.status.idle":"2022-04-17T14:25:29.463046Z","shell.execute_reply.started":"2022-04-17T14:25:29.458429Z","shell.execute_reply":"2022-04-17T14:25:29.462131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's find cryptocurrencies. First, we have to make sure that the data is correct and we have no missing values\n\n### <a id='2'>2. Dealing wtih Missing Data</a>","metadata":{}},{"cell_type":"code","source":"# Count missing data\ndf_missing = df.isna().sum().sort_values(ascending=False)\ndf_missing[df_missing > 0]","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:29.465808Z","iopub.execute_input":"2022-04-17T14:25:29.466014Z","iopub.status.idle":"2022-04-17T14:25:29.490397Z","shell.execute_reply.started":"2022-04-17T14:25:29.465989Z","shell.execute_reply":"2022-04-17T14:25:29.489352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only 5 cryptocurrencies have missing data. Let's remove them.","metadata":{}},{"cell_type":"code","source":"df_missing_list = df_missing[df_missing > 0].index.tolist()\ndf_missing_list","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:29.492366Z","iopub.execute_input":"2022-04-17T14:25:29.492627Z","iopub.status.idle":"2022-04-17T14:25:29.500273Z","shell.execute_reply.started":"2022-04-17T14:25:29.492595Z","shell.execute_reply":"2022-04-17T14:25:29.499377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(columns = df_missing_list)\ncrypto_codes_list = df.columns.tolist()\ncrypto_codes_list.remove('date')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:29.501508Z","iopub.execute_input":"2022-04-17T14:25:29.502129Z","iopub.status.idle":"2022-04-17T14:25:29.513108Z","shell.execute_reply.started":"2022-04-17T14:25:29.502098Z","shell.execute_reply":"2022-04-17T14:25:29.512564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TASK :** Try not to delete cryptocurrencies with missing data, but to impute or interpolate them to neighboring values","metadata":{}},{"cell_type":"code","source":"# MinMaxScaler\ndef df_minmax_scaler(df):\n    # Data Scalling\n    scaler = MinMaxScaler().fit(df)\n    df = pd.DataFrame(scaler.transform(df), columns = df.columns)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:29.514027Z","iopub.execute_input":"2022-04-17T14:25:29.514677Z","iopub.status.idle":"2022-04-17T14:25:29.525617Z","shell.execute_reply.started":"2022-04-17T14:25:29.514651Z","shell.execute_reply":"2022-04-17T14:25:29.52466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df2 = df[['date', 'BTC', 'ETH', 'USDT', 'BNB']].copy()\ndf2 = df.copy()\ndf2.index = df['date']\ndf2 = df2.drop(columns=['date'])\ndf2 = df_minmax_scaler(df2)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:29.52677Z","iopub.execute_input":"2022-04-17T14:25:29.527007Z","iopub.status.idle":"2022-04-17T14:25:29.544162Z","shell.execute_reply.started":"2022-04-17T14:25:29.526974Z","shell.execute_reply":"2022-04-17T14:25:29.543082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The 5 cryptocurrencies with the biggest market cap\ncrypto_codes_list_biggest = crypto_codes_list[:5]\ndf2[crypto_codes_list_biggest].plot(figsize=(16,12))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:29.545539Z","iopub.execute_input":"2022-04-17T14:25:29.545891Z","iopub.status.idle":"2022-04-17T14:25:29.978902Z","shell.execute_reply.started":"2022-04-17T14:25:29.545859Z","shell.execute_reply":"2022-04-17T14:25:29.978255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The 5 cryptocurrencies with the biggest market cap\n#axs = df2.plot.area(figsize=(12, len(crypto_codes_list)), subplots=True)\naxs = df2[crypto_codes_list_biggest].plot.area(figsize=(12, 5), subplots=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:29.979964Z","iopub.execute_input":"2022-04-17T14:25:29.980355Z","iopub.status.idle":"2022-04-17T14:25:30.692278Z","shell.execute_reply.started":"2022-04-17T14:25:29.980315Z","shell.execute_reply":"2022-04-17T14:25:30.691219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3 = df.melt(id_vars = ['date'])\ndf3.columns = ['date', 'currency', 'value']\ndf3","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:30.693814Z","iopub.execute_input":"2022-04-17T14:25:30.694072Z","iopub.status.idle":"2022-04-17T14:25:30.715592Z","shell.execute_reply.started":"2022-04-17T14:25:30.694034Z","shell.execute_reply":"2022-04-17T14:25:30.714829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='3'>3. Time Series Feature Extraction</a>\n\nIn general, time series clustering can be divided into 2 types:\n- **Feature-Based approach**: we try to extract everything possible from the signal/time series (feature extraction)\n- **Raw data-Based approach**: directly applied to time series vectors without any spatial transformations\n\nIn this notebook, we are going to use **Raw-data Based approach**. It means that we will have a matrix of features where:\n- Rows: Different Time Series\n- Features: Time Observations\n\nIn this case, we will be clustering in a very high dimensional space and will most likely run into a problem known as the **Curse of Dimensionality**. As a result, obtained clusters may have sparse shapes, overlap with other clusters and so on.\n\nTo prevent this, we will need to use **dimensionality reduction methods** (t-SNE, PCA, MDS...)\n\n### <a id='4'>4. Clustering Methods</a>\n\nWe will focus on the following clustering methods:\n- `K-Means/TimeSeriesKMeans: (ts_learn library)`\n- `Hierarchical Agglomerative Clustering`\n\nBut any known clustering algorithm can be applied\n\n**If the time series is a signal** (data from various devices), then the best way to extract features would be methods from the `signal processing` area\n\nFor example, Fourier transformation for finding different frequencies, spectrograms and wavelet transformations\n\n**If the series is noisy, then it would be nice to smooth it first** (various smoothing methods) so as not to find false patterns\n\n### <a id='4.1'>4.1 Time Series Smoothing</a>\nNice, we don't have missing values **but the series is still looking noisy**. Let's apply moving average (window size = 7: weekly trend )","metadata":{}},{"cell_type":"code","source":"# Time Series Smoothing \nres_df = pd.DataFrame()\nfor item in df3['currency'].unique():\n    current_cur = df3.query(f'currency == \"{item}\"')\n    current_cur['smoth_7'] = current_cur['value'].rolling(7, center=True).mean()\n    res_df = res_df.append(current_cur[['date', 'currency', 'smoth_7']])\n    \ndf4 = res_df.dropna()\ndf4","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:30.716722Z","iopub.execute_input":"2022-04-17T14:25:30.717268Z","iopub.status.idle":"2022-04-17T14:25:31.180853Z","shell.execute_reply.started":"2022-04-17T14:25:30.717235Z","shell.execute_reply":"2022-04-17T14:25:31.179949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's have a look to Bitcoin\nselected_cur3 = df3[df3['currency']=='BTC']\nselected_cur4 = df4[df4['currency']=='BTC']\nselected_cur3['smooth'] = selected_cur4['smoth_7']\nselected_cur3[['value', 'smooth']].plot(figsize=(12, 6))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:31.182257Z","iopub.execute_input":"2022-04-17T14:25:31.1826Z","iopub.status.idle":"2022-04-17T14:25:31.472603Z","shell.execute_reply.started":"2022-04-17T14:25:31.182558Z","shell.execute_reply":"2022-04-17T14:25:31.471919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After smoothing we can get more insights about the series as well as define similarities between them\n\nInitial preprocessing has been done and we can create the main feature matrix  ","metadata":{}},{"cell_type":"code","source":"# Feature matrix with shape (n_series x time_observations)\nseries_df = df4.pivot(index='currency', columns='date', values='smoth_7')\nseries_df = series_df.dropna(axis='columns')\nseries_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:31.47376Z","iopub.execute_input":"2022-04-17T14:25:31.473983Z","iopub.status.idle":"2022-04-17T14:25:31.520209Z","shell.execute_reply.started":"2022-04-17T14:25:31.473956Z","shell.execute_reply":"2022-04-17T14:25:31.51934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='4.2'>4.2 Time Series Scaling</a>\nScaling must be applied to each series independently","metadata":{}},{"cell_type":"code","source":"# Scaling\nscaler = StandardScaler()\n\n# First transposition - to have series in columns (allows scaling each series independently)\n# Second Transposition - come back to initial feature matrix shape (n_series x time_observations)\nscaler = StandardScaler()\nscaled_ts = scaler.fit_transform(series_df.T).T ","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:31.521683Z","iopub.execute_input":"2022-04-17T14:25:31.521981Z","iopub.status.idle":"2022-04-17T14:25:31.534242Z","shell.execute_reply.started":"2022-04-17T14:25:31.521941Z","shell.execute_reply":"2022-04-17T14:25:31.533448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='4.3'>4.3 Time Series K-Means</a>\n\nWhen using `K-Means` clustering, it is better to use the **Feature-Based Approach**. We extract a bunch of features from the series and hope that they will describe the time series well then perform clustering. I'd like to demonstrate **out of the box solution** (Raw-Data Approach). For Feature-Based Approach, you have to get features for each series and group them using any clustering algorithm. These libraries can help: \n- <a href='https://github.com/fraunhoferportugal/tsfel'>ts_fel</a>\n- <a href='https://github.com/blue-yonder/tsfresh'>ts_fresh</a>\n\nIt is important how we define the similarity between observations in a feature space. When using KMeans we can use:\n- `Euclidean distance` \n- `Dynamic Time Warping Matching (DTW)`\n\n\nWhen using <a href='https://tslearn.readthedocs.io/en/stable/user_guide/dtw.html'> Dynamic Time Warping Matching </a> the **Feature-Based approach is not suitable**, since we are trying to determine a measure of the similarity of the series (how they overlap, peaks size/similarity/location...)\n\nFor `DTW` better downsample the series using `resampling` (i.e. change the frequency of the series). For example, instead of daily observations/ticks, take 5/10/15 days ones but we have to keep in mind that the main patterns (peaks, fluctuations) fall into this interval. It allows keeping the series structure, making it shorter and therefore much faster to identify similar series with `DTW`\n\n\nFirst, apply KMeans algorithm from <a href='https://tslearn.readthedocs.io/en/stable/index.html'>ts_learn library</a>","metadata":{}},{"cell_type":"code","source":"# Run KMeans and plot the results \ndef get_kmeans_results(data, max_clusters=10, metric='euclidean', seed=23):\n    \"\"\"\n    Runs KMeans n times (according to max_cluster range)\n\n    data: pd.DataFrame or np.array\n        Time Series Data\n    max_clusters: int\n        Number of different clusters for KMeans algorithm\n    metric: str\n        Distance metric between the observations\n    seed: int\n        random seed\n    Returns: \n    -------\n    None      \n    \"\"\"\n    # Main metrics\n    distortions = []\n    silhouette = []\n    clusters_range = range(1, max_clusters+1)\n    \n    for K in tqdm(clusters_range):\n        kmeans_model = TimeSeriesKMeans(n_clusters=K, metric=metric, n_jobs=-1, max_iter=10, random_state=seed)\n        kmeans_model.fit(data)\n        distortions.append(kmeans_model.inertia_)\n        if K > 1:\n            silhouette.append(silhouette_score(data, kmeans_model.labels_))\n        \n    # Visualization\n    plt.figure(figsize=(10,4))\n    plt.plot(clusters_range, distortions, 'bx-')\n    plt.xlabel('k')\n    plt.ylabel('Distortion')\n    plt.title('Elbow Method')\n    \n    plt.figure(figsize=(10,4))\n    plt.plot(clusters_range[1:], silhouette, 'bx-')\n    plt.xlabel('k')\n    plt.ylabel('Silhouette score')\n    plt.title('Silhouette');","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:31.535598Z","iopub.execute_input":"2022-04-17T14:25:31.536037Z","iopub.status.idle":"2022-04-17T14:25:31.810582Z","shell.execute_reply.started":"2022-04-17T14:25:31.535993Z","shell.execute_reply":"2022-04-17T14:25:31.809927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try finding similar series using DTW metric","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Run the algorithm using DTW algorithm \nget_kmeans_results(data=scaled_ts, max_clusters=5, metric='dtw', seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:25:31.811445Z","iopub.execute_input":"2022-04-17T14:25:31.811966Z","iopub.status.idle":"2022-04-17T14:26:42.798465Z","shell.execute_reply.started":"2022-04-17T14:25:31.811939Z","shell.execute_reply":"2022-04-17T14:26:42.797095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, we can hardly say anything according to Silhouette (4?) but Elbow Method says that 2 clusters are good\n\nLet's have a look at obtained clusters ","metadata":{}},{"cell_type":"code","source":"# Visualization for obtained clusters   \ndef plot_clusters(data, cluster_model, dim_red_algo, xsize=16, ysize=10, title=\"\"):\n    \"\"\"\n    Plots clusters obtained by clustering model \n\n    data: pd.DataFrame or np.array\n        Time Series Data\n    cluster_model: Class\n        Clustering algorithm \n    dim_red_algo: Class\n        Dimensionality reduction algorithm (e.g. TSNE/PCA/MDS...) \n    Returns:\n    -------\n    None\n    \"\"\"\n    cluster_labels = cluster_model.fit_predict(data)\n    centroids = cluster_model.cluster_centers_\n    u_labels = np.unique(cluster_labels)\n    \n    # Centroids Visualization\n    plt.figure(figsize=(xsize, ysize))\n    plt.scatter(centroids[:, 0] , centroids[:, 1] , s=150, color='r', marker=\"x\")\n    \n    # Downsize the data into 2D\n    if data.shape[1] > 2:\n        data_2d = dim_red_algo.fit_transform(data)\n        for u_label in u_labels:\n            cluster_points = data[(cluster_labels == u_label)]\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=u_label)\n    else:\n        for u_label in u_labels:\n            cluster_points = data[(cluster_labels == u_label)]\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=u_label)\n\n    plt.title('Clustered Data'+title)\n    plt.xlabel(\"Feature space for the 1st feature\")\n    plt.ylabel(\"Feature space for the 2nd feature\")\n    plt.grid(True)\n    plt.legend(title='Cluster Labels');","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:44:18.632389Z","iopub.execute_input":"2022-04-17T14:44:18.63268Z","iopub.status.idle":"2022-04-17T14:44:18.645005Z","shell.execute_reply.started":"2022-04-17T14:44:18.632645Z","shell.execute_reply":"2022-04-17T14:44:18.644221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# let's look at the cluster shape with n_clusters=2 (Elbow Method)\nmodel = TimeSeriesKMeans(n_clusters=2, metric='dtw', n_jobs=-1, max_iter=10, random_state=SEED)\n\nplot_clusters(data=scaled_ts,\n              cluster_model=model,\n              dim_red_algo=TSNE(n_components=2, init='pca', random_state=SEED))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:26:42.813557Z","iopub.execute_input":"2022-04-17T14:26:42.814515Z","iopub.status.idle":"2022-04-17T14:27:04.190344Z","shell.execute_reply.started":"2022-04-17T14:26:42.814469Z","shell.execute_reply":"2022-04-17T14:27:04.189137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# let's look at the cluster shape with n_clusters=4 (Silhouette Method)\nmodel = TimeSeriesKMeans(n_clusters=4, metric='dtw', n_jobs=-1, max_iter=10, random_state=SEED)\n\nplot_clusters(data=scaled_ts,\n              cluster_model=model,\n              dim_red_algo=TSNE(n_components=2, init='pca', random_state=SEED))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:29:48.239497Z","iopub.execute_input":"2022-04-17T14:29:48.239807Z","iopub.status.idle":"2022-04-17T14:29:59.566642Z","shell.execute_reply.started":"2022-04-17T14:29:48.239772Z","shell.execute_reply":"2022-04-17T14:29:59.565158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clusters overlap and cluster number 2 or 4 looks like a noise","metadata":{}},{"cell_type":"code","source":"# let's compare with the euclidean metric\nget_kmeans_results(data=scaled_ts, max_clusters=5, metric='euclidean', seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:04.191914Z","iopub.execute_input":"2022-04-17T14:27:04.192244Z","iopub.status.idle":"2022-04-17T14:27:05.007448Z","shell.execute_reply.started":"2022-04-17T14:27:04.192202Z","shell.execute_reply":"2022-04-17T14:27:05.006692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results are much worse in comparison with `DTW` algorithm. Let's try downsizing the features\n\n### <a id='4.4'>4.4 Downsizing Feature Space</a> \n\nLet's apply dimensionality reduction methods (t-SNE, MDS, VRAE...)\n\n### <a id='4.4.1'>4.4.1 t-SNE</a> ","metadata":{}},{"cell_type":"code","source":"# Downsize the features into 2D\ntsne = TSNE(n_components=2, init='pca', random_state=SEED)\ndata_tsne = tsne.fit_transform(scaled_ts)\n\nget_kmeans_results(data=data_tsne, max_clusters=10, metric='euclidean', seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:05.0086Z","iopub.execute_input":"2022-04-17T14:27:05.008782Z","iopub.status.idle":"2022-04-17T14:27:06.316292Z","shell.execute_reply.started":"2022-04-17T14:27:05.008758Z","shell.execute_reply":"2022-04-17T14:27:06.315418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's look at the cluster shape\nmodel = TimeSeriesKMeans(n_clusters=2, metric='euclidean', n_jobs=-1, max_iter=10, random_state=SEED)\n\nplot_clusters(data=data_tsne,\n              cluster_model=model,\n              dim_red_algo=TSNE(n_components=2, init='pca', random_state=SEED))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:06.320007Z","iopub.execute_input":"2022-04-17T14:27:06.320281Z","iopub.status.idle":"2022-04-17T14:27:06.673601Z","shell.execute_reply.started":"2022-04-17T14:27:06.320247Z","shell.execute_reply":"2022-04-17T14:27:06.672698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cluster shape is relatively good, observations don't overlap but are a bit sparse\n\n### <a id='4.4.2'>4.4.2 MultiDimensional Scaling (MDS)</a> ","metadata":{}},{"cell_type":"code","source":"mds = MDS(n_components=2, n_init=3, max_iter=100, random_state=SEED)\ndata_mds = mds.fit_transform(scaled_ts) \n\nget_kmeans_results(data=data_mds, max_clusters=10, metric='euclidean', seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:06.674899Z","iopub.execute_input":"2022-04-17T14:27:06.675098Z","iopub.status.idle":"2022-04-17T14:27:07.810491Z","shell.execute_reply.started":"2022-04-17T14:27:06.675074Z","shell.execute_reply":"2022-04-17T14:27:07.80945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's look at the cluster shape\nmodel = TimeSeriesKMeans(n_clusters=2, metric='euclidean', n_jobs=-1, max_iter=10, random_state=SEED)\n\nplot_clusters(data=data_mds,\n              cluster_model=model,\n              dim_red_algo=TSNE(n_components=2, init='pca', random_state=SEED))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:37:45.82088Z","iopub.execute_input":"2022-04-17T14:37:45.821897Z","iopub.status.idle":"2022-04-17T14:37:46.19415Z","shell.execute_reply.started":"2022-04-17T14:37:45.821846Z","shell.execute_reply":"2022-04-17T14:37:46.19344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can choose between 2 and 5 clusters","metadata":{}},{"cell_type":"code","source":"# let's look at the cluster shape\nfor i in range(4):\n    model = TimeSeriesKMeans(n_clusters=i+2, metric='euclidean', n_jobs=-1, max_iter=10, random_state=SEED)\n\n    plot_clusters(data=data_mds,\n                  cluster_model=model,\n                  dim_red_algo=TSNE(n_components=2, init='pca', random_state=SEED), xsize=12, ysize=4, \n                  title=f'for {i+2} clusters')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:44:32.303716Z","iopub.execute_input":"2022-04-17T14:44:32.303988Z","iopub.status.idle":"2022-04-17T14:44:33.99483Z","shell.execute_reply.started":"2022-04-17T14:44:32.303956Z","shell.execute_reply":"2022-04-17T14:44:33.993503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='4.5'>4.5 Hierarchical Agglomerative Clustering (HAC)</a> ","metadata":{}},{"cell_type":"code","source":"# HAC clustering (similar to get_kmeans_results function)\ndef get_hac_results(data, max_clusters=10, linkage='euclidean', seed=23):\n    silhouette = []\n    clusters_range = range(2, max_clusters+1)\n    for K in tqdm(clusters_range):\n        model = AgglomerativeClustering(n_clusters=K, linkage=linkage)\n        model.fit(data)\n        silhouette.append(silhouette_score(data, model.labels_))\n        \n    # Plot\n    plt.figure(figsize=(10,4))\n    plt.plot(clusters_range, silhouette, 'bx-')\n    plt.xlabel('k')\n    plt.ylabel('Silhouette score')\n    plt.title('Silhouette')\n    plt.grid(True);","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:08.269033Z","iopub.execute_input":"2022-04-17T14:27:08.26931Z","iopub.status.idle":"2022-04-17T14:27:08.276663Z","shell.execute_reply.started":"2022-04-17T14:27:08.269275Z","shell.execute_reply":"2022-04-17T14:27:08.275417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at all results at a time \nfeatures_df = [scaled_ts, data_tsne, data_mds]\nfor df in features_df:\n    get_hac_results(data=df, max_clusters=10, linkage='ward', seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:08.27788Z","iopub.execute_input":"2022-04-17T14:27:08.278113Z","iopub.status.idle":"2022-04-17T14:27:09.313246Z","shell.execute_reply.started":"2022-04-17T14:27:08.278082Z","shell.execute_reply":"2022-04-17T14:27:09.312158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's choose 5 clusters with MDS features","metadata":{}},{"cell_type":"code","source":"def plot_dendrogram(data, model, figsize=(16,10), **kwargs):\n    \"\"\"\n    Plots a dendogram using HAC \n\n    data: pd.DataFrame or np.array\n        Time Series Data\n    model: Class\n        Clustering Model \n    figsize: tuple\n        Figure size\n    Returns:\n    -------\n    None \n    \"\"\"\n    model.fit(data)\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  \n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n    \n    plt.figure(figsize=figsize, dpi=200)\n    dendrogram(linkage_matrix, **kwargs)\n    plt.title('Dendogram')\n    plt.xlabel('Objects')\n    plt.ylabel('Distance')\n    plt.grid(False)\n    plt.tight_layout();","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:09.314758Z","iopub.execute_input":"2022-04-17T14:27:09.31508Z","iopub.status.idle":"2022-04-17T14:27:09.32736Z","shell.execute_reply.started":"2022-04-17T14:27:09.315025Z","shell.execute_reply":"2022-04-17T14:27:09.326162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dendrogram\nmodel = AgglomerativeClustering(n_clusters=5, linkage='ward', affinity='euclidean', compute_distances=True)\n\nplot_dendrogram(data=features_df[-1],\n                model=model,\n                color_threshold=60)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:57:41.134335Z","iopub.execute_input":"2022-04-17T14:57:41.135222Z","iopub.status.idle":"2022-04-17T14:57:43.319721Z","shell.execute_reply.started":"2022-04-17T14:57:41.135143Z","shell.execute_reply":"2022-04-17T14:57:43.318832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  <a id='4.6'>4.6 Time Series KMeans Results</a> \nFinally, we will choose TimeSeriesKMeans with downsized features using MDS and 5 clusters. It's likely that the data is various and with 5 clusters we will get clusters with similar series.","metadata":{}},{"cell_type":"code","source":"# Train TimeSeriesKMeans with MDS\nkmeans_model = TimeSeriesKMeans(n_clusters=5, metric='euclidean', n_jobs=-1, max_iter=10, random_state=SEED)\ncluster_labels = kmeans_model.fit_predict(data_mds)\n\nts_clustered = [ scaled_ts[(cluster_labels == lable), :] for lable in np.unique(cluster_labels)]","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:11.822448Z","iopub.execute_input":"2022-04-17T14:27:11.82276Z","iopub.status.idle":"2022-04-17T14:27:11.858325Z","shell.execute_reply.started":"2022-04-17T14:27:11.82273Z","shell.execute_reply":"2022-04-17T14:27:11.857373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Objects distribution in the obtained clusters \nlabels = [f'Cluster_{i}' for i in range(len(ts_clustered))]\nsamples_in_cluster = [val.shape[0] for val in ts_clustered]\n\nplt.figure(figsize=(16,5))\nplt.bar(labels, samples_in_cluster);","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:11.859503Z","iopub.execute_input":"2022-04-17T14:27:11.859965Z","iopub.status.idle":"2022-04-17T14:27:12.047539Z","shell.execute_reply.started":"2022-04-17T14:27:11.859937Z","shell.execute_reply":"2022-04-17T14:27:12.046426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def plot_cluster_ts(current_cluster):\n    \"\"\"\n    Plots time series in a cluster \n\n    current_cluster: np.array\n        Cluster with time series \n    Returns:\n    -------\n    None \n    \"\"\"\n    fig, ax = plt.subplots(\n        int(np.ceil(current_cluster.shape[0]/4)),4,\n        figsize=(45, 3*int(np.ceil(current_cluster.shape[0]/4)))\n    )\n    fig.autofmt_xdate(rotation=45)\n    ax = ax.reshape(-1)\n    for indx, series in enumerate(current_cluster):\n        ax[indx].plot(series)\n        plt.xticks(rotation=45)\n\n    plt.tight_layout()\n    plt.show();","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:12.048648Z","iopub.execute_input":"2022-04-17T14:27:12.048847Z","iopub.status.idle":"2022-04-17T14:27:12.054878Z","shell.execute_reply.started":"2022-04-17T14:27:12.048822Z","shell.execute_reply":"2022-04-17T14:27:12.054116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at the obtained clusters","metadata":{}},{"cell_type":"code","source":"for cluster in range(len(ts_clustered)):\n    print(f\"==========Cluster number: {cluster}==========\")\n    plot_cluster_ts(ts_clustered[cluster])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:12.055773Z","iopub.execute_input":"2022-04-17T14:27:12.05595Z","iopub.status.idle":"2022-04-17T14:27:22.943771Z","shell.execute_reply.started":"2022-04-17T14:27:12.055928Z","shell.execute_reply":"2022-04-17T14:27:22.943084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the series within its cluster are alike and it is cool. We have found out that all the cryptocurrencies can be clustered into 5 different groups. There are cryptocurrencies that have the same patterns \n\n### <a id='5'>5. Cluster Series Extraction</a>\nAlright, we clustered the series data, what's next? Well, it depends on the task you are dealing with. Probably, after clustering the series you will want to get a cluster series (a series that describes all the series in the cluster)\n\nThere are several options:\n- Use cluster centroid \n- Take the mean of all the series in a cluster\n- Takes a series that has a minimum distance to the cluster centroid \n- <a href='https://tslearn.readthedocs.io/en/stable/variablelength.html#barycenter-computation'>DBA method</a>\n\nWe will cover:\n- DBA\n- Cluster Mean\n- Closest Series to Cluster Centroid ","metadata":{}},{"cell_type":"code","source":"# Closest Series to Cluster Centroid\nclosest_clusters_indxs = [np.argmin([np.linalg.norm(cluster_center - point, ord=2) for point in data_mds]) \\\n                                                                        for cluster_center in kmeans_model.cluster_centers_]\n\nclosest_ts = scaled_ts[closest_clusters_indxs, :]","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:22.944806Z","iopub.execute_input":"2022-04-17T14:27:22.945107Z","iopub.status.idle":"2022-04-17T14:27:22.962517Z","shell.execute_reply.started":"2022-04-17T14:27:22.945079Z","shell.execute_reply":"2022-04-17T14:27:22.961576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DBA\ndba_ts = [dtw_barycenter_averaging(cluster_serieses, max_iter=10, verbose=True) for cluster_serieses in ts_clustered]","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:22.96381Z","iopub.execute_input":"2022-04-17T14:27:22.964072Z","iopub.status.idle":"2022-04-17T14:27:24.598288Z","shell.execute_reply.started":"2022-04-17T14:27:22.964038Z","shell.execute_reply":"2022-04-17T14:27:24.597632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's compare how a certain method affects a final cluster shape\n\nChoose a cluster with a few series. This will help to see the differences between the algorithms!","metadata":{}},{"cell_type":"code","source":"CLUSTER_N = 2\n\nplt.figure(figsize=(25, 5))\nplt.plot(ts_clustered[CLUSTER_N].T,  alpha = 0.4) # all series in the cluster_1\nplt.plot(closest_ts[CLUSTER_N], c = 'r', label='Cluster Time Series')\nplt.title('Cluster Series - Closest to Cluster Centroid. Cluster 1')\nplt.legend();\n\nplt.figure(figsize=(25, 5))\nplt.plot(ts_clustered[CLUSTER_N].T,  alpha = 0.4) \nplt.plot(np.mean(ts_clustered[CLUSTER_N], axis=0), c = 'r', label='Cluster Time Series')\nplt.title('Cluster Series - Cluster Mean. Cluster 1')\nplt.legend();\n\nplt.figure(figsize=(25, 5))\nplt.plot(ts_clustered[CLUSTER_N].T,  alpha = 0.4) \nplt.plot(dba_ts[CLUSTER_N], c = 'r', label='Cluster Time Series')\nplt.title('Cluster Series - DBA. Cluster 1')\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:24.599682Z","iopub.execute_input":"2022-04-17T14:27:24.600107Z","iopub.status.idle":"2022-04-17T14:27:25.67522Z","shell.execute_reply.started":"2022-04-17T14:27:24.60007Z","shell.execute_reply":"2022-04-17T14:27:25.674194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Why not choose the first option? Well, it has a big spike and doesn't describe all series data. As a solution, smoothing can be applied (I think it's always a good idea to apply smoothing in this case because noisy series might be chosen)\n\nDBA or Mean method look good. Both can be chosen!\n\n### <a id='5.1'>5.1 Cluster Series DBA</a>\nAll clusters series extracted by DBA","metadata":{}},{"cell_type":"code","source":"for indx, series in enumerate(dba_ts):\n    plt.figure(figsize=(25, 5))\n    plt.plot(ts_clustered[indx].T,  alpha = 0.15)\n    plt.plot(series, c = 'r', label='Cluster Time Series')\n    plt.title(f'Scaled values. Cluster {indx}')\n    plt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:25.676619Z","iopub.execute_input":"2022-04-17T14:27:25.677629Z","iopub.status.idle":"2022-04-17T14:27:27.210201Z","shell.execute_reply.started":"2022-04-17T14:27:25.677584Z","shell.execute_reply":"2022-04-17T14:27:27.208939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='6'>6. Time Series Embeddings</a>\n\nIn this approach, we will train NN (Recurrent Auto-encoders with LSTM / GRU blocks) and get compressed vector representations of series (embeddings)\n\nTrying to train the encoder and decoder in such a way that in all the variety of data at the input they would receive series close to each other, and those that differ were separated, according to the distance that we choose.\n\nThe algorithm is trained in unsupervised mode. Obtained embeddings will be clustered in the end ","metadata":{}},{"cell_type":"code","source":"os.chdir('./timeseries-clustering-vae')\n\nfrom vrae.vrae import VRAE\nfrom vrae.utils import *\n\nimport torch\nimport plotly\nfrom torch.utils.data import DataLoader, TensorDataset\nplotly.offline.init_notebook_mode()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:27.211626Z","iopub.execute_input":"2022-04-17T14:27:27.211877Z","iopub.status.idle":"2022-04-17T14:27:27.662198Z","shell.execute_reply.started":"2022-04-17T14:27:27.211843Z","shell.execute_reply":"2022-04-17T14:27:27.661197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vrae_df = scaled_ts.copy()\ndload = '/content/timeseries_clustering_vae/' ","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:27.663672Z","iopub.execute_input":"2022-04-17T14:27:27.663959Z","iopub.status.idle":"2022-04-17T14:27:27.669711Z","shell.execute_reply.started":"2022-04-17T14:27:27.663925Z","shell.execute_reply":"2022-04-17T14:27:27.668649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Params\nhidden_size = 50\nhidden_layer_depth = 1\nlatent_length = 20\nbatch_size = 5\nlearning_rate = 0.005\nn_epochs = 40\ndropout_rate = 0.1\noptimizer = 'Adam' # Adam/SGD\ncuda = True # Train on GPU\nprint_every=30\nclip = True \nmax_grad_norm=5\nloss = 'MSELoss' # SmoothL1Loss/MSELoss\nblock = 'LSTM' # LSTM/GRU","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:27.671218Z","iopub.execute_input":"2022-04-17T14:27:27.671513Z","iopub.status.idle":"2022-04-17T14:27:27.684063Z","shell.execute_reply.started":"2022-04-17T14:27:27.671476Z","shell.execute_reply":"2022-04-17T14:27:27.683037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We don't use test_df, create train_df using all the data we have\nX_train = np.expand_dims(scaled_ts, -1)\ntrain_dataset = TensorDataset(torch.from_numpy(X_train))\n\nsequence_length = X_train.shape[1] \nnumber_of_features = X_train.shape[2] ","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:27.685359Z","iopub.execute_input":"2022-04-17T14:27:27.685719Z","iopub.status.idle":"2022-04-17T14:27:27.69907Z","shell.execute_reply.started":"2022-04-17T14:27:27.685689Z","shell.execute_reply":"2022-04-17T14:27:27.698079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Creation \nvrae = VRAE(sequence_length=sequence_length,\n            number_of_features = number_of_features,\n            hidden_size = hidden_size, \n            hidden_layer_depth = hidden_layer_depth,\n            latent_length = latent_length,\n            batch_size = batch_size,\n            learning_rate = learning_rate,\n            n_epochs = n_epochs,\n            dropout_rate = dropout_rate,\n            optimizer = optimizer, \n            cuda = cuda,\n            print_every=print_every, \n            clip=clip, \n            max_grad_norm=max_grad_norm,\n            loss = loss,\n            block = block,\n            dload = dload)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:27.70066Z","iopub.execute_input":"2022-04-17T14:27:27.701097Z","iopub.status.idle":"2022-04-17T14:27:27.713789Z","shell.execute_reply.started":"2022-04-17T14:27:27.701053Z","shell.execute_reply":"2022-04-17T14:27:27.712757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nvrae.fit(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:27:27.715878Z","iopub.execute_input":"2022-04-17T14:27:27.716332Z","iopub.status.idle":"2022-04-17T14:28:47.611774Z","shell.execute_reply.started":"2022-04-17T14:27:27.7163Z","shell.execute_reply":"2022-04-17T14:28:47.61055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get embeddings\nembeddings = vrae.transform(train_dataset)\n\n# Cluster the embeddings\nget_kmeans_results(data=embeddings, max_clusters=10, metric='euclidean', seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:28:47.613319Z","iopub.execute_input":"2022-04-17T14:28:47.613515Z","iopub.status.idle":"2022-04-17T14:28:48.71551Z","shell.execute_reply.started":"2022-04-17T14:28:47.61349Z","shell.execute_reply":"2022-04-17T14:28:48.714984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TimeSeriesKMeans(n_clusters=6, metric='euclidean', n_jobs=-1, max_iter=10, random_state=SEED)\n \nplot_clusters(data=embeddings,\n              cluster_model=model,\n              dim_red_algo=TSNE(n_components=2, init='pca', random_state=SEED))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T14:48:04.677926Z","iopub.execute_input":"2022-04-17T14:48:04.678264Z","iopub.status.idle":"2022-04-17T14:48:05.567712Z","shell.execute_reply.started":"2022-04-17T14:48:04.678227Z","shell.execute_reply":"2022-04-17T14:48:05.566534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\n[Go to Top](#0)","metadata":{}}]}